{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7e00af",
   "metadata": {},
   "source": [
    "# üìä –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã:\n",
    "- –¢–æ–ø —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤\n",
    "- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç\n",
    "- –ê—Ñ—Ñ–∏–Ω–∏—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–ª–æ–≤ –í—É–¥—Ö–∞—É—Å–∞ –∫ –∫–æ—Ä–ø—É—Å—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_DIR = Path(\"../data/wodehouse\")\n",
    "CORPUS_DIR = Path(\"../data/gutenberg\")\n",
    "\n",
    "def load_words(pathlist):\n",
    "    words = []\n",
    "    for f in pathlist:\n",
    "        with open(f, encoding='utf-8') as file:\n",
    "            words.extend(file.read().lower().split())\n",
    "    return words\n",
    "\n",
    "author_words = load_words(AUTHOR_DIR.glob(\"*.txt\"))\n",
    "print(f\"‚úçÔ∏è  –°–ª–æ–≤ —É –∞–≤—Ç–æ—Ä–∞: {len(author_words):,}\")\n",
    "\n",
    "if CORPUS_DIR.exists():\n",
    "    corpus_words = load_words(CORPUS_DIR.glob(\"*.txt\"))\n",
    "    print(f\"üåç –°–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len(corpus_words):,}\")\n",
    "else:\n",
    "    corpus_words = []\n",
    "    print(\"‚ö†Ô∏è  –ü–∞–ø–∫–∞ —Å –∫–æ—Ä–ø—É—Å–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c76f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_freq = Counter(author_words)\n",
    "corpus_freq = Counter(corpus_words)\n",
    "\n",
    "auth_total = sum(auth_freq.values())\n",
    "corpus_total = sum(corpus_freq.values())\n",
    "\n",
    "rows = []\n",
    "for word, count in auth_freq.items():\n",
    "    if corpus_freq[word] > 0 and count > 5:\n",
    "        rel_auth = count / auth_total\n",
    "        rel_corp = corpus_freq[word] / corpus_total\n",
    "        affinity = rel_auth / rel_corp\n",
    "        rows.append((word, count, corpus_freq[word], affinity))\n",
    "\n",
    "aff_df = pd.DataFrame(rows, columns=[\"word\", \"author_count\", \"corpus_count\", \"affinity\"])\n",
    "aff_df = aff_df.sort_values(\"affinity\", ascending=False)\n",
    "aff_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053aad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(np.log10(aff_df[\"affinity\"] + 1e-9), bins=50)\n",
    "plt.title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞ –∞—Ñ—Ñ–∏–Ω–∏—Ç–∏–≤–Ω–æ—Å—Ç–∏\")\n",
    "plt.xlabel(\"log10(Affinity)\")\n",
    "plt.ylabel(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_df.to_csv(\"../data/wodehouse_affinity.csv\", index=False)\n",
    "print(\"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ data/wodehouse_affinity.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
