# build_index.py
# üåê –°—Ç—Ä–æ–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è ChromaDB

from pathlib import Path
from tqdm import tqdm
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
import hashlib
import re

CORPUS_PATH = Path("/workspace/data")
DB_PATH = "/workspace/chroma_db"

# üìÅ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
chroma_client = chromadb.PersistentClient(path=DB_PATH)
embedding_fn = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

collection = chroma_client.get_or_create_collection(
    name="gutenberg-index",
    embedding_function=embedding_fn,
    metadata={"creator": "wordcracker"}
)

def extract_metadata(path: Path):
    """–í—ã–¥–µ–ª–∏—Ç—å –∞–≤—Ç–æ—Ä–∞, –Ω–∞–∑–≤–∞–Ω–∏–µ, —è–∑—ã–∫ –∏ ID –∫–Ω–∏–≥–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    parts = path.parts[-2:]
    author = parts[0]
    title = path.stem
    file_hash = hashlib.md5(path.read_bytes()).hexdigest()[:12]
    return {
        "author": author,
        "title": title,
        "hash": file_hash,
        "filepath": str(path)
    }

def chunk_text(text: str, max_words=200):
    words = text.split()
    for i in range(0, len(words), max_words):
        chunk = words[i:i + max_words]
        yield " ".join(chunk)

print("‚ôªÔ∏è –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤...")

for txt_file in tqdm(list(CORPUS_PATH.rglob("*.txt"))):
    text = txt_file.read_text(encoding="utf-8", errors="ignore")
    metadata = extract_metadata(txt_file)

    for i, chunk in enumerate(chunk_text(text)):
        uid = f"{metadata['hash']}_{i}"
        collection.add(
            documents=[chunk],
            ids=[uid],
            metadatas=[{
                "author": metadata["author"],
                "title": metadata["title"],
                "filepath": metadata["filepath"],
                "chunk": i
            }]
        )

print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {collection.count()}")
